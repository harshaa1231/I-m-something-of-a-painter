{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":194.628113,"end_time":"2024-12-11T22:22:30.783315","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-11T22:19:16.155202","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Importing Libraries**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nimport zipfile\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-12-13T01:28:00.059884Z","iopub.execute_input":"2024-12-13T01:28:00.060225Z","iopub.status.idle":"2024-12-13T01:28:00.064955Z","shell.execute_reply.started":"2024-12-13T01:28:00.060195Z","shell.execute_reply":"2024-12-13T01:28:00.064100Z"},"papermill":{"duration":4.652224,"end_time":"2024-12-11T22:19:23.368159","exception":false,"start_time":"2024-12-11T22:19:18.715935","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Implementing a U-Net Style generator from scratch.**","metadata":{}},{"cell_type":"code","source":"# Generator (U-Net Style)\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        # Encoder: Downsampling layers\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1), # First layer: Input channels = 3 (RGB), Output channels = 64\n            nn.ReLU(True), # Activation function for non-linearity\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),# Second layer: Halves the spatial dimensions\n            nn.BatchNorm2d(128),# Batch normalization for stable training and faster convergence\n            nn.ReLU(True),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),# Third layer: Further halves the spatial dimension\n            nn.BatchNorm2d(256),\n            nn.ReLU(True)\n        )\n\n         # Decoder: Upsampling layers\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),# Upsamples the spatial dimensions by 2\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),# Second upsampling layer\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            # Final layer: Restores the original dimensions, 3 channels (RGB)\n            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        # Pass input through encoder to extract features\n        x = self.encoder(x)\n        # Pass the extracted features through decoder to reconstruct the output\n        x = self.decoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-12-13T02:28:18.494679Z","iopub.execute_input":"2024-12-13T02:28:18.495031Z","iopub.status.idle":"2024-12-13T02:28:18.502580Z","shell.execute_reply.started":"2024-12-13T02:28:18.495001Z","shell.execute_reply":"2024-12-13T02:28:18.501669Z"},"papermill":{"duration":0.019403,"end_time":"2024-12-11T22:19:23.391982","exception":false,"start_time":"2024-12-11T22:19:23.372579","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"**Implementing a PatchGAN discriminator.**","metadata":{}},{"cell_type":"code","source":"# Discriminator (PatchGAN)\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        # PatchGAN discriminator model\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, True), # LeakyReLU activation to prevent dying neurons\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),# Batch normalization for stable training and faster convergence\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),# Further halves spatial dimensions\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, True),# Final layer: Outputs a single channel (real or fake prediction)\n            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=1)\n        )\n\n    def forward(self, x):\n        # Forward pass through the PatchGAN model\n        return self.model(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T02:36:19.088846Z","iopub.execute_input":"2024-12-13T02:36:19.089613Z","iopub.status.idle":"2024-12-13T02:36:19.095319Z","shell.execute_reply.started":"2024-12-13T02:36:19.089581Z","shell.execute_reply":"2024-12-13T02:36:19.094407Z"},"papermill":{"duration":0.015527,"end_time":"2024-12-11T22:19:23.412682","exception":false,"start_time":"2024-12-11T22:19:23.397155","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"**Initializing the models**","metadata":{}},{"cell_type":"code","source":"# Initializing Models\ngenerator = Generator()\ndiscriminator = Discriminator()\n\n# Checking if CUDA (GPU) is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Moving the models to selected device\ngenerator = generator.to(device)\ndiscriminator = discriminator.to(device)\n\n# Loss Functions and Optimizers\nadversarial_loss = nn.MSELoss() # for adversarial training\ncycle_loss = nn.L1Loss()  # L1 loss for cycle consistency\nidentity_loss = nn.L1Loss() # L1 loss for identity mapping\n\n#Optimizers for Generator and discriminators\noptimizer_G = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))","metadata":{"execution":{"iopub.status.busy":"2024-12-13T01:28:00.089909Z","iopub.execute_input":"2024-12-13T01:28:00.090152Z","iopub.status.idle":"2024-12-13T01:28:00.119027Z","shell.execute_reply.started":"2024-12-13T01:28:00.090116Z","shell.execute_reply":"2024-12-13T01:28:00.118385Z"},"papermill":{"duration":0.304728,"end_time":"2024-12-11T22:19:23.721718","exception":false,"start_time":"2024-12-11T22:19:23.416990","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Preparing the datset for training**","metadata":{}},{"cell_type":"code","source":"# Dataset Preparation\ndata_transforms = transforms.Compose([\n    transforms.Resize((256, 256)), # Resizing images\n    transforms.ToTensor(), # Converting images to PyTorch tensors\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # Normalizing pixel values to range [-1, 1]\n])\n\ndata_path = '/kaggle/input/gan-getting-started/'\n\nphotos_path = os.path.join(data_path, 'photo_jpg')\nmonet_path = os.path.join(data_path, 'monet_jpg')\n\nphoto_files = [os.path.join(photos_path, file) for file in os.listdir(photos_path)]\nmonet_files = [os.path.join(monet_path, file) for file in os.listdir(monet_path)]\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T01:28:00.120804Z","iopub.execute_input":"2024-12-13T01:28:00.121062Z","iopub.status.idle":"2024-12-13T01:28:00.139476Z","shell.execute_reply.started":"2024-12-13T01:28:00.121035Z","shell.execute_reply":"2024-12-13T01:28:00.138846Z"},"papermill":{"duration":0.095203,"end_time":"2024-12-11T22:19:23.819855","exception":false,"start_time":"2024-12-11T22:19:23.724652","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Preprocessing the Images from the dataset**","metadata":{}},{"cell_type":"code","source":"# A custom dataset class for loading flat images\nclass FlatImageDataset(Dataset):\n    def __init__(self, file_paths, transform=None):\n        self.file_paths = file_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.file_paths[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img\n\n# Datasets for the photos and Monet-style images\nphoto_dataset = FlatImageDataset(photo_files, transform=data_transforms)\nmonet_dataset = FlatImageDataset(monet_files, transform=data_transforms)\n\n# Data loaders for the datasets\nphoto_loader = DataLoader(photo_dataset, batch_size=16, shuffle=True)\nmonet_loader = DataLoader(monet_dataset, batch_size=2, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-13T01:28:00.140464Z","iopub.execute_input":"2024-12-13T01:28:00.140739Z","iopub.status.idle":"2024-12-13T01:28:00.146905Z","shell.execute_reply.started":"2024-12-13T01:28:00.140713Z","shell.execute_reply":"2024-12-13T01:28:00.145908Z"},"papermill":{"duration":0.011313,"end_time":"2024-12-11T22:19:23.833841","exception":false,"start_time":"2024-12-11T22:19:23.822528","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training the dataset with the images,gave it 30 epochs**","metadata":{}},{"cell_type":"code","source":"# Training Loop\nnum_epochs = 30 # Reduced epochs for Kaggle competition\nfor epoch in range(num_epochs):\n    for i, (photo_batch, monet_batch) in enumerate(zip(photo_loader, monet_loader)):\n        # Move data to device\n        real_photos = photo_batch.to(device)\n        real_monets = monet_batch.to(device)\n\n        # Train Generator\n        optimizer_G.zero_grad()\n\n        # Generating fake Monet images from real photos\n        fake_monets = generator(real_photos)\n         # Getting discriminator's validity prediction for the fake Monet images\n        validity = discriminator(fake_monets)\n        g_loss = adversarial_loss(validity, torch.ones_like(validity)) + \\\n                 cycle_loss(generator(fake_monets), real_photos) + \\\n                 identity_loss(generator(real_photos), real_photos)\n        g_loss.backward()\n        optimizer_G.step()\n\n        # Training Discriminator and computing discriminator's prediction for real and fake Monet images\n        optimizer_D.zero_grad()\n        real_validity = discriminator(real_monets)\n        fake_validity = discriminator(fake_monets.detach())\n        \n        d_loss = (adversarial_loss(real_validity, torch.ones_like(real_validity)) + \\\n                 adversarial_loss(fake_validity, torch.zeros_like(fake_validity))) / 2\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Print Progress\n        if i % 10 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i}], G_Loss: {g_loss.item()}, D_Loss: {d_loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-13T01:28:00.148219Z","iopub.execute_input":"2024-12-13T01:28:00.148503Z","iopub.status.idle":"2024-12-13T01:42:21.494985Z","shell.execute_reply.started":"2024-12-13T01:28:00.148474Z","shell.execute_reply":"2024-12-13T01:42:21.494036Z"},"papermill":{"duration":131.520984,"end_time":"2024-12-11T22:21:35.357550","exception":false,"start_time":"2024-12-11T22:19:23.836566","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Visualising the predictions**","metadata":{}},{"cell_type":"code","source":"# Visualize Predictions\nimport matplotlib.pyplot as plt\ndef visualize_predictions(loader, generator, num_images=5):\n    generator.eval()\n    with torch.no_grad():\n        for i, photo_batch in enumerate(loader):\n            real_photos = photo_batch.to(device)\n             # Generating Monet-style images from real photos\n            fake_monets = generator(real_photos)\n            fake_monets = (fake_monets * 0.5 + 0.5).clamp(0, 1)  # Denormalize to [0, 1]\n\n            for j in range(min(num_images, len(fake_monets))):\n                real_img = transforms.ToPILImage()(real_photos[j].cpu())\n                fake_img = transforms.ToPILImage()(fake_monets[j].cpu())\n\n                 # Plotting real photo and generated Monet side by side\n\n                plt.figure(figsize=(8, 4))\n                plt.subplot(1, 2, 1)\n                plt.title(\"Real Photo\")\n                plt.imshow(real_img)\n                plt.axis(\"off\")\n\n                plt.subplot(1, 2, 2)\n                plt.title(\"Generated Monet\")\n                plt.imshow(fake_img)\n                plt.axis(\"off\")\n                plt.show()\n\n                num_images -= 1\n                if num_images <= 0:\n                    return\n\nvisualize_predictions(photo_loader, generator)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-13T01:42:21.497026Z","iopub.execute_input":"2024-12-13T01:42:21.497307Z","iopub.status.idle":"2024-12-13T01:42:22.935611Z","shell.execute_reply.started":"2024-12-13T01:42:21.497280Z","shell.execute_reply":"2024-12-13T01:42:22.934706Z"},"papermill":{"duration":1.568333,"end_time":"2024-12-11T22:21:36.931886","exception":false,"start_time":"2024-12-11T22:21:35.363553","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate and Save Images\noutput_dir = \"../tmp/generated_images\"\nos.makedirs(output_dir, exist_ok=True)\n# zip file to store generated images\nwith zipfile.ZipFile(\"images.zip\", \"w\") as zipf:\n    image_counter = 1  # Initialize a global counter for unique file names\n    for i, photo_batch in enumerate(photo_loader):\n        real_photos = photo_batch.to(device)\n        fake_monets = generator(real_photos)\n        fake_monets = (fake_monets * 0.5 + 0.5).clamp(0, 1)  # Denormalize to [0, 1]\n\n        for j, img in enumerate(fake_monets):\n            img = transforms.ToPILImage()(img.cpu())\n            img_path = os.path.join(output_dir, f\"image_{image_counter}.jpg\")\n            img.save(img_path)\n            zipf.write(img_path, os.path.basename(img_path))\n            image_counter += 1  # Increment the counter for each image\n\n            if image_counter > 8000:  # Stop after generating 8000 images\n                break\n        if image_counter > 8000:\n            break","metadata":{"execution":{"iopub.status.busy":"2024-12-13T01:42:22.936758Z","iopub.execute_input":"2024-12-13T01:42:22.937028Z","iopub.status.idle":"2024-12-13T01:43:03.974918Z","shell.execute_reply.started":"2024-12-13T01:42:22.937002Z","shell.execute_reply":"2024-12-13T01:43:03.973865Z"},"papermill":{"duration":52.067756,"end_time":"2024-12-11T22:22:29.031403","exception":false,"start_time":"2024-12-11T22:21:36.963647","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.046775,"end_time":"2024-12-11T22:22:29.132342","exception":false,"start_time":"2024-12-11T22:22:29.085567","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.045888,"end_time":"2024-12-11T22:22:29.224232","exception":false,"start_time":"2024-12-11T22:22:29.178344","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.045873,"end_time":"2024-12-11T22:22:29.315715","exception":false,"start_time":"2024-12-11T22:22:29.269842","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}